from gneiss.regression._sparse import sparse_matmul, sparse_clr, minibatch
from gneiss.balances import sparse_balance_basis
from edward.models import Normal, Poisson, PointMass
from skbio.stats.composition import _gram_schmidt_basis
from gneiss.util import sparse_match_tips


def bayesian_ols(formula, table, metadata, tree=None,
                 alpha_mean=0, alpha_scale=1,
                 theta_mean=0, theta_scale=1,
                 gamma_mean=0, gamma_scale=1,
                 beta_mean=0, beta_scale=1,
                 iterations=1000, learning_rate=1e-1,
                 beta1=0.9, beta2=0.99,
                 batch_size=1000, seed=None,
                 check_nans=False,
                 summary_dir=None):
    """ Bayesian poisson regression.

    Parameters
    ----------
    formula : str
        Formula representing the statistical equation to be evaluated.
        These strings are similar to how equations are handled in R and
        statsmodels. Note that the dependent variable in this string should
        not be specified, since this method will be run on each of the
        individual balances. See `patsy` for more details.
    table : pd.DataFrame
        Contingency table where samples correspond to rows and
        balances correspond to columns.
    metadata: pd.DataFrame
        Metadata table that contains information about the samples contained
        in the `table` object.  Samples correspond to rows and covariates
        correspond to columns.
    tree: skbio.TreeNode
        Tree to specify the ILR basis (optional).  If this is not specified,
        then the default gram schmidt basis will be used.
    alpha_mean : float
        Mean of the global scale prior distribution.  This is used to
        correct for global sequencing depth bias. (default: 0)
    alpha_scale : float
        Scale of the global scale prior distribution.  This is used to
        correct for global sequencing depth bias. (default: 1)
    theta_mean : float
        Mean of the sample scale prior distribution.  This is used to
        correct for sample specific sequencing depth sequencing bias.
        (default: 0)
    theta_scale : float
        Scale of the sample scale prior distribution.  This is used to
        correct for sample specific sequencing depth sequencing bias.
        (default: 1)
    gamma_mean : float
        Mean of the feature scale prior distribution.  These are intercept
        terms for each species. (default: 0)
    gamma_scale : float
        Scale of the feature scale prior distribution.  These are intercept
        terms for each species. (default: 1)
    beta_mean : float
        Mean of the coefficient prior distribution.  These are the
        regression coefficients of interest. (default: 0)
    beta_scale : float
        Scale of the coefficient prior distribution.  These are the
        regression coefficients of interest. (default: 1)
    iterations : int
        Number of iterations to perform in the stochastic gradient descent.
        (default: 1000)
    learning_rate : float
        Learning rate for stochastic gradient optimization.
        (default: 1e-1)
    beta1 : float
        First Momentum rate parameter for ADAM optimizer. (default: 0.9)
    beta2 : float
        Second Momentum rate parameter for ADAM optimizer. (default: 0.99)
    seed : int
        Random seed for Tensorflow. (default: None)
    check_nan : bool
        Specifies if nans should be checked at each step of the tensorflow
        computation graph.  Note that the can drastically slow down
        calculations (default: False)
    summary_dir : str
        Location of diectory to sort summmaries about all of the parameters.
        This can be later visualized in tensorflow.


    """
    # TODO: Need to add checks for samples
    table, tree = sparse_match_tips(table, tree)

    if tree is None:
        basis = coo_matrix(_gram_schmidt_basis(num_features))
    else:
        basis = sparse_balance_basis(tree)[0].todense()


class BayesianOLS():

    def fit(self, Y, X, basis):

        # TODO: need to add in formula conversion from metadata
        N, D = num_samples, num_features
        p = metadata.shape[1]   # number of covariates

        # dummy variables for mini-batch size
        batch_row = tf.placeholder(tf.int32, shape=[batch_size], name='batch_i')
        batch_col = tf.placeholder(tf.int32, shape=[batch_size], name='batch_j')

        # global bias
        alpha = Normal(loc=tf.zeros([]) + alpha_mean,
                       scale=tf.ones([]) * alpha_scale,
                       name='alpha')

        # sample bias
        theta = Normal(loc=tf.zeros([N, 1]) + theta_mean,
                       scale=tf.ones([N, 1]) * theta_scale,
                       name='theta')
        # species bias
        gamma = Normal(loc=tf.zeros([1, D-1]) + gamma_mean,
                       scale=tf.ones([1, D-1]) * gamma_scale,
                       name='gamma')

        # dummy variable for gradient
        G = tf.placeholder(tf.float32, [N, p], name='G')
        # add bias terms for samples
        Gprime = tf.concat([theta, tf.ones([N, 1]), G], axis=1)

        # Specify regression coefficents
        B = Normal(loc=tf.zeros([p, D-1]) + beta_mean,
                   scale=tf.ones([p, D-1]) * beta_scale,
                   name='B')

        # add bias terms for features
        Bprime = tf.concat([tf.ones([1, D-1]), gamma, B], axis=0)

        # Convert basis to SparseTensor
        psi = tf.SparseTensor(
            indices=np.mat([basis.row, basis.col]).transpose(),
            values=basis.data,
            dense_shape=basis.shape)

        # clr transform coefficients first
        Vprime = sparse_clr(Bprime, psi)
        #Vprime = tf.concat([tf.ones([1, D]), V], axis=0)
        # retrieve entries selected by index
        eta = sparse_matmul(
            Gprime, Vprime,
            row_index=batch_row, col_index=batch_col
        )
        # obtain counts
        Y = Poisson( rate=tf.exp(eta + alpha), name='Y' )


        # These are the posterior distributions.
        tf.set_random_seed(seed)

        qalpha = PointMass(
            params=tf.Variable(tf.random_normal([])) ,
            name='qalpha')

        qgamma = PointMass(
            params=tf.Variable(tf.random_normal([1, D-1])) ,
            name='qgamma')

        qtheta = PointMass(
            params=tf.Variable(tf.random_normal([N, 1])),
            name='qtheta')

        qB = PointMass(
            params=tf.Variable(tf.random_normal([p, D-1])) ,
            name='qB')


        # a placeholder for the microbial counts
        # since we will be manually feeding it into the inference via minibatch SGD
        Y_ph = tf.placeholder(tf.float32, shape=[batch_size], name='Y_placeholder')

        inference = ed.MAP({
            theta: qtheta,
            alpha: qalpha,
            gamma: qgamma,
            B: qB},
            data={G: metadata.values, Y: Y_ph}
        )

        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,
                                           beta1=beta1,
                                           beta2=beta2)


        # adds checks for nans
        tf.add_check_numerics_ops()

        sess = ed.get_session()
        saver = tf.train.Saver()
        inference.initialize(n_iter=iterations,
                             optimizer=optimizer,
                             n_print=100,
                             log_vars=[
                                 qB,
                                 qtheta,
                             ],
                             logdir=summary_dir)

        # initialize all tensorflow variables
        tf.global_variables_initializer().run()

        for i in range(inference.n_iter):

            # get batches
            idx_row, idx_col, idx_data = minibatch(M=batch_size, Y=data, p=0.4)

            info_dict = inference.update(
                feed_dict={batch_row: idx_row, batch_col: idx_col, Y_ph: idx_data})
            inference.print_progress(info_dict)


        if summary_dir is not None:
            tf.summary.tensor_summary('qB', qB)
            tf.summary.tensor_summary('theta', qtheta)
            tf.summary.scalar('beta_mean', beta_mean)
            tf.summary.scalar('beta_scale', beta_scale)
            tf.summary.scalar('theta_mean', theta_mean)
            tf.summary.scalar('theta_scale', theta_scale)
            tf.summary.scalar('iteration', iterations)
            tf.summary.scalar('learning_rate', learning_rate)
            tf.summary.scalar('batch_size', batch_size)
            tf.summary.scalar('beta1', beta1)
            tf.summary.scalar('beta2', beta2)
