"""
Provides tfserve main (script) support.
"""

import argparse
import importlib
import socket
import sys

from werkzeug.exceptions import BadRequest

from tfserve.tfserve import TFServeApp
from tfserve import helper

DEFAULT_HANDLER = 'json'
DEFAULT_HOST = '0.0.0.0'
DEFAULT_PORT = 5000

DESCRIPTION = """

TFServe
-------

Serve TF models simple and easy as an HTTP API

"""


EPILOG = """

ABOUT HANDLERS

  Processing of HTTP requests and responses are handled according
  to the --handler argument.

  The following handlers are currently supported by TFServe:

    json  Requests handled as JSON model inputs.

  You may alternative specify a Python module name for
  --handler. See CUSTOM HANDLERS below for details.

  * JSON HANDLER: Inputs are submitted to the root path '/'
  using HTTP POST and contain a JSON encoded map of input tensors to input
  values. Input values must comply with the expected tensors
  shape. If batch mode is enabled, multiple input values must be
  submitted in an array. A response body is a JSON encoded map of
  output tensors to output values generated by the model for the given the inputs.
  If batch mode is enabled, output values are returned in an array.

  * CUSTOM HANDLERS: A Python module name may be specified with --handler. The module
  must contain a `create_handler(**kw)` function that returns an
  object that implements `tfserve.EncodeDecodeHandler`. Keywords
  used in calls to `create_handler` are:

        inputs   List of inputs specified with --inputs.
        outputs  List of outputs specified with --outputs.
        batch    Boolean indicating whether handler should process
                 inputs as batches rather than as single inputs.


MODEL HELP

  For help with model input and output tensors, use --help-model.

  Note that in order to use the --help-model you will need to provide the
  model path with the `-m` or `--model` argument.


ABOUT TFSERVE

For more information, issues or suggestions, go to https://github.com/iitzco/tfserve

"""

def main():
    """TFServe main function.

    """
    maybe_help_args = _init_args(require_tensors=False)
    if maybe_help_args.help_model:
        _show_model_help_and_exit(maybe_help_args)
    serve_args = _init_args(require_tensors=True)
    _serve_model(serve_args)

def _init_args(require_tensors):
    p = argparse.ArgumentParser(
        description=DESCRIPTION,
        epilog=EPILOG,
        formatter_class=argparse.RawTextHelpFormatter,
        add_help=False)
    p.add_argument(
        '-m', '--model', metavar='PATH', required=True,
        help="path to pb file or directory containing checkpoint")
    p.add_argument(
        '-i', '--inputs', required=require_tensors,
        help="a comma separated list of input tensors")
    p.add_argument(
        '-o', '--outputs', required=require_tensors,
        help="a comma separated list of output tensors")
    p.add_argument(
        '-h', '--handler', default=DEFAULT_HANDLER,
        help="encode/decode handler (deault is '%s')" % DEFAULT_HANDLER)
    p.add_argument(
        '-b', '--batch', action='store_true',
        help=(
            "process multiple inputs (default is to process\n"
            "one input per request)"))
    p.add_argument(
        '-H', '--host', default=DEFAULT_HOST,
        help="host interface to bind to (%s)" % DEFAULT_HOST)
    p.add_argument(
        '-p', '--port', default=DEFAULT_PORT,
        help="port to listen on (%i)" % DEFAULT_PORT)
    p.add_argument(
        '--help-model', action='store_true',
        help=(
            "show model help for PATH and exit; use to list\n"
            "possible input and output tensors"))
    p.add_argument(
        '--help', action='help',
        help="show this help message and exit")
    return p.parse_args()

def _show_model_help_and_exit(args):
    helper.estimate_io_tensors(args.model)
    sys.exit(0)

def _serve_model(args):
    inputs = _split_tensors(args.inputs)
    outputs = _split_tensors(args.outputs)
    handler = _init_handler(inputs, outputs, args)
    app = TFServeApp(
        args.model,
        inputs,
        outputs,
        handler.encode,
        handler.decode,
        args.batch)
    sys.stdout.write("Using %s\n" % handler.get_description())
    sys.stdout.write("Running at %s\n" % _serve_url(args))
    app.run(args.host, args.port, _middleware)
    sys.stdout.write('\n')

def _split_tensors(tensors):
    return [s.strip() for s in tensors.split(',')]

def _init_handler(inputs, outputs, args):
    possible_handler_modules = [
        'tfserve.%s_handler' % args.handler,
        args.handler,
    ]
    for mod_name in possible_handler_modules:
        try:
            handler_mod = importlib.import_module(mod_name)
        except ImportError:
            pass
        else:
            return handler_mod.create_handler(
                inputs=inputs,
                outputs=outputs,
                batch=args.batch)
    raise SystemExit(
        "tfserve: unsupported handler '%s'\n"
        "Try 'tfserve --help' for a list of supported handlers."
        % args.handler)

def _serve_url(args):
    host = args.host
    if not host or host == "0.0.0.0":
        host = socket.gethostname()
        try:
            # Verify that configured hostname is valid
            socket.gethostbyname(host)
        except socket.gaierror:
            host = 'localhost'
    return 'http://%s:%i' % (host, args.port)

def _middleware(handler, req):
    try:
        return handler(req)
    except ValueError as e:
        raise BadRequest(str(e))

if __name__ == '__main__':
    main()
