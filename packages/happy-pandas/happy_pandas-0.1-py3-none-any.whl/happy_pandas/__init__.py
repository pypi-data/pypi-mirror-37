import happybase as hb
import pandas as pd
import pyspark
from collections import defaultdict


def toDF(host: str, port: int, tbl_name: str, column_family='verified',
         limit=None, exclude_mcc=[5999, 6010, 6011]) -> pd.core.frame.DataFrame:
    """
    Retrieves the verified user-to-mcc transactions from hbase and stores it to a pandas.Dataframe
    where the index is the user id, the columns are the mccs and the values are the transaction frequency.

    :param host: hbase host
    :param port: hbase port
    :param tbl_name: hbase table name
    :param column_family: hbase column family name of verified transactions
    :param limit: number of rows to fetch
    :param exclude_mcc: list of mccs to exclude
    :return: pandas.Dataframe
    """
    conn = hb.Connection(host, port)
    table = conn.table(tbl_name)
    data_dict = defaultdict(dict)
    for row, data in table.scan(columns=[column_family], limit=limit):
        user_id = row.decode("utf-8")
        for verified_mcc, freq in data.items():
            mcc = int(verified_mcc.decode('utf-8').replace(f"{column_family}:", ""))
            rating = int(freq.decode('utf-8'))
            data_dict[user_id].update({mcc: rating})

    df = pd.DataFrame.from_dict(data_dict, orient='index')

    try:
        df_drop_exclude_mcc = df.drop(columns=exclude_mcc)
    except KeyError:
        df_drop_exclude_mcc = df

    df_drop_all_na = df_drop_exclude_mcc.dropna(how='all')

    final_df = df_drop_all_na.fillna(0)

    return final_df


def toSparkDF(spark: pyspark.sql.SparkSession, host: str, port: int, tbl_name: str,
              column_family='verified', limit=None, exclude_mcc=[5999, 6010, 6011],
              user_col='user', item_col='item', rating_col='rating') -> pyspark.sql.dataframe.DataFrame:
    """
    Retrieves the verified user-to-mcc transactions from hbase and stores it to a pyspark.sql.dataframe.DataFrame
    :param spark: SparkSession used to create the DataFrame
    :param host: hbase host
    :param port: hbase port
    :param tbl_name: hbase table name
    :param column_family: hbase column family name of verified transactions
    :param limit: number of rows to fetch
    :param exclude_mcc: list of mccs to exclude
    :param user_col: dataframe user column
    :param item_col: dataframe items column
    :param rating_col: dataframe ratings column
    :return: pyspark.sql.dataframe.DataFrame
    """
    df = toDF(host, port, tbl_name, column_family, limit, exclude_mcc)
    user_item_rating = []
    for user, row in df.iterrows():
        for item, rating in row.iteritems():
            if rating:
                user_item_rating.append((user, item, rating))

    final_df = spark.createDataFrame(user_item_rating, [user_col, item_col, rating_col])
    return final_df


def toHB(df: pyspark.sql.dataframe.DataFrame, host: str, port: int, tbl_name: str, column_family: str,
         index_to_user_id_map = dict(), batch_size=1000, user_col='user', recommendations_col='recommendations',
         item_col='item', rating_col='rating'):
    """
    Uploads predictions generated by pyspark.ml.recommendation.ALSModel.recommendForAllUsers to HBase

    :param df: Dataframe generated from recommendForAllUsers
    :param host: hbase host
    :param port: hbase port
    :param tbl_name: hbase table name
    :param column_family: hbase column family name for generated predictions
    :param index_to_user_id_map: mapping from index to UUID
    :param batch_size: hbase upload batch size
    :param user_col: dataframe user column generated from recommendForAllUsers
    :param recommendations_col: dataframe recommendations column generated from recommendForAllUsers
    :param item_col: dataframe items column generated from recommendForAllUsers
    :param rating_col: dataframe ratings column generated from recommendForAllUsers
    :return: None
    """

    def f(rows):
        conn = hb.Connection(host, port)
        table = conn.table(tbl_name)
        b = table.batch(batch_size=batch_size)
        for row in rows:
            for recom in row[recommendations_col]:
                if index_to_user_id_map:
                    row_user = index_to_user_id_map[row[user_col]]
                else:
                    row_user = str(row[user_col])
                b.put(row_user, {f'{column_family}:{recom[item_col]}': str(recom[rating_col])})
        b.send()
        conn.close()

    df.foreachPartition(f)
