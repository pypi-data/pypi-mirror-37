import happybase as hb
import pandas as pd
from collections import defaultdict


def toDF(host, port, tbl_name, size=5, column_family='verified',
         limit=None, exclude_mcc=[5999, 6010, 6011], timeout=5, batch_size=1000):
    """
    Retrieves the verified user-to-mcc transactions from hbase and stores it to a pandas.Dataframe
    where the index is the user id, the columns are the mccs and the values are the transaction frequency.

    :param host: hbase host
    :param port: hbase port
    :param tbl_name: hbase table name
    :param size: the maximum number of concurrently open connections
    :param column_family: hbase column family name of verified transactions
    :param limit: number of rows to fetch
    :param exclude_mcc: list of mccs to exclude
    :param timeout: number of seconds to wait for the connection
    :param batch_size: how many results should be retrieved per batch when retrieving results from the scanner.
    :return: pandas.Dataframe
    """
    pool = hb.ConnectionPool(size, host=host, port=port)
    with pool.connection(timeout) as conn:
        table = conn.table(tbl_name)
        data_dict = defaultdict(dict)
        for row, data in table.scan(columns=[column_family], batch_size=batch_size, limit=limit):
            user_id = row.decode("utf-8")
            for verified_mcc, freq in data.items():
                mcc = int(verified_mcc.decode('utf-8').replace("{}:".format(column_family), ""))
                rating = int(freq.decode('utf-8'))
                data_dict[user_id].update({mcc: rating})

    df = pd.DataFrame.from_dict(data_dict, orient='index')

    try:
        df_drop_exclude_mcc = df.drop(columns=exclude_mcc)
    except KeyError:
        df_drop_exclude_mcc = df

    df_drop_all_na = df_drop_exclude_mcc.dropna(how='all')

    final_df = df_drop_all_na.fillna(0)

    return final_df


def toSparkDF(spark, host, port, tbl_name,
              column_family='verified', limit=None, exclude_mcc=[5999, 6010, 6011],
              user_col='user', item_col='item', rating_col='rating'):
    """
    Retrieves the verified user-to-mcc transactions from hbase and stores it to a pyspark.sql.dataframe.DataFrame
    :param spark: SparkSession used to create the DataFrame
    :param host: hbase host
    :param port: hbase port
    :param tbl_name: hbase table name
    :param column_family: hbase column family name of verified transactions
    :param limit: number of rows to fetch
    :param exclude_mcc: list of mccs to exclude
    :param user_col: dataframe user column
    :param item_col: dataframe items column
    :param rating_col: dataframe ratings column
    :return: pyspark.sql.dataframe.DataFrame
    """
    df = toDF(host, port, tbl_name, column_family, limit, exclude_mcc)
    user_item_rating = []
    for user, row in df.iterrows():
        for item, rating in row.iteritems():
            if rating:
                user_item_rating.append((user, item, rating))

    final_df = spark.createDataFrame(user_item_rating, [user_col, item_col, rating_col])
    return final_df


def toHB(df, host, port, tbl_name, column_family,
         index_to_user_id_map = dict(), batch_size=1000, user_col='user', recommendations_col='recommendations',
         item_col='item', rating_col='rating'):
    """
    Uploads predictions generated by pyspark.ml.recommendation.ALSModel.recommendForAllUsers to HBase

    :param df: Dataframe generated from recommendForAllUsers
    :param host: hbase host
    :param port: hbase port
    :param tbl_name: hbase table name
    :param column_family: hbase column family name for generated predictions
    :param index_to_user_id_map: mapping from index to UUID
    :param batch_size: hbase upload batch size
    :param user_col: dataframe user column generated from recommendForAllUsers
    :param recommendations_col: dataframe recommendations column generated from recommendForAllUsers
    :param item_col: dataframe items column generated from recommendForAllUsers
    :param rating_col: dataframe ratings column generated from recommendForAllUsers
    :return: None
    """

    def f(rows):
        conn = hb.Connection(host, port)
        table = conn.table(tbl_name)
        b = table.batch(batch_size=batch_size)
        for row in rows:
            for recom in row[recommendations_col]:
                if index_to_user_id_map:
                    row_user = index_to_user_id_map[row[user_col]]
                else:
                    row_user = str(row[user_col])
                b.put(row_user, {'{}:{}'.format(column_family, recom[item_col]): str(recom[rating_col])})
        b.send()
        conn.close()

    df.foreachPartition(f)
